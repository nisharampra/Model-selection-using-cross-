{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection using cross-validation\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "* Go through the notebook and complete the tasks.\n",
    "* Make sure you understand the examples given. If you need help, refer to the documentation links provided or go to the discussion forum..\n",
    "* When a question allows a free-form answer (e.g. what do you observe?), create a new markdown cell below and answer the question in the notebook. \n",
    "* Save your notebooks when you are done. \n",
    "\n",
    "In this lab, we will work through using cross-validation.\n",
    "\n",
    "**Task 1:**\n",
    "In the cell below, we load the iris dataset and show the indices recovered by using a built-in cross-validation function from scikit-learn. These indices can be then used for splitting our data into a training set and testing set. Note that while in this example we use StratifiedShuffleSplit, there are many other cross-validation generators in scikit-learn. You can read more about this <a href=\"https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\">here</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train indices: [9 8 4 3 6 2 7 1] test indices: [5 0]\n",
      "train indices: [1 6 8 3 0 2 4 5] test indices: [9 7]\n",
      "train indices: [8 7 3 5 1 9 4 6] test indices: [0 2]\n",
      "train indices: [5 0 9 8 6 2 3 1] test indices: [4 7]\n",
      "train indices: [5 4 6 0 1 2 7 3] test indices: [8 9]\n",
      "train indices: [8 6 2 0 7 3 1 5] test indices: [4 9]\n",
      "train indices: [7 8 5 3 9 6 4 2] test indices: [1 0]\n",
      "train indices: [4 8 5 1 6 7 9 3] test indices: [2 0]\n",
      "train indices: [8 9 0 4 6 1 5 7] test indices: [3 2]\n",
      "train indices: [6 2 4 1 7 8 9 5] test indices: [3 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Set X as a samples times features matrix, Y equal to the targets\n",
    "# Use only the first 10 datapoints for this example\n",
    "X = iris.data[0:10]\n",
    "y = iris.target[0:10]\n",
    "\n",
    "# Use StratifiedShuffleSplit for cross-validation\n",
    "cvsplt = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in cvsplt.split(X, y):\n",
    "    print(\"train indices:\", train_index, \"test indices:\", test_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "In the cell below, we have the sample code that was used to train a k-NN classifier on the iris dataset. Your task is to use the code above to change the classification process we show below, in order to perform cross-validation on the data and obtain an estimate of the performance of the classifier with 10 neighbours on the unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for each fold: [1.0, 1.0, 0.9, 0.9666666666666667, 0.9666666666666667, 0.9, 1.0, 0.9333333333333333, 0.9666666666666667, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Set X as a samples times features matrix, Y equal to the targets\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Use StratifiedShuffleSplit for cross-validation\n",
    "cvsplt = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Train and evaluate the model using cross-validation\n",
    "for train_index, test_index in cvsplt.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Define k-NN classifier with 10 neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "\n",
    "    # Fit the classifier\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict values for test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(\"Accuracies for each fold:\", accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 3:**  Now you are going to attempt to write your own cross-validation function to evaluate the k-NN classifier above (i.e. not using StratifiedShuffleSplit). \n",
    "\n",
    "Again, we are using a fixed distance (euclidean) and a fixed number of neighbours (10) so we do **not** need to create a validation set.\n",
    "\n",
    "Your function (see cell below) firstly splits the indices of each of our data into bins according to the number of folds (here: 5-fold).\n",
    "\n",
    "Then, you should loop through all folds, split the data into training and testing by selecting the appropriate bins (see slides on cross-validation), train on training data and save the test result as the accuracy for each fold (see list accuracy_fold). This is the list that your function should return in the end. Remember that the extend function extends a list with more values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for each fold: [1.0, 1.0, 0.9333333333333333, 0.9333333333333333, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def myCrossVal(X, y, foldK):\n",
    "    accuracy_fold = []  # list to store accuracies for each fold\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(len(y))\n",
    "    \n",
    "    # Split indices into k different bins\n",
    "    bins = np.array_split(indices, foldK)\n",
    "\n",
    "    # Loop through folds\n",
    "    for i in range(foldK):\n",
    "        foldTrain = []  # list to save current indices for training\n",
    "        foldTest = bins[i]  # current bin for testing\n",
    "\n",
    "        # Collect training indices (all other bins)\n",
    "        for j in range(foldK):\n",
    "            if j != i:\n",
    "                foldTrain.extend(bins[j])\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        X_train = X[foldTrain]\n",
    "        X_test = X[foldTest]\n",
    "        y_train = y[foldTrain]\n",
    "        y_test = y[foldTest]\n",
    "\n",
    "        # Train k-NN classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Test on test data\n",
    "        y_pred = knn.predict(X_test)\n",
    "\n",
    "        # Append accuracy to the list\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_fold.append(accuracy)\n",
    "\n",
    "    return accuracy_fold\n",
    "\n",
    "# Perform cross-validation and get accuracies\n",
    "accuracy_fold = myCrossVal(X, y, 5)\n",
    "print(\"Accuracies for each fold:\", accuracy_fold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Print the average accuracy and standard deviation of your results over the 5 folds. (functions ``mean`` and ``std``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.9733333333333334\n",
      "Standard Deviation of Accuracy: 0.03265986323710903\n"
     ]
    }
   ],
   "source": [
    "# Calculate average accuracy and standard deviation\n",
    "average_accuracy = np.mean(accuracy_fold)\n",
    "std_deviation = np.std(accuracy_fold)\n",
    "\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Standard Deviation of Accuracy:\", std_deviation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross validation\n",
    "Performing **nested** cross validation is slightly more elaborate, but necessary when we want to learn the best values for our parameters (e.g. here, finding the best values for the number of neighbours that k-NN should have). You can follow the examples on scikit-learn <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\">here</a> and <a href=\"https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\">here</a> to learn how to do this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
